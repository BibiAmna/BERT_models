# -*- coding: utf-8 -*-
"""BERTModels_for_Named_Entity_Recognition_BC5CDR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1liRfCrIf3AA7rmz1PfMt2goxkMYltF-l

# Named Entity Recognition on BC5CDR (Chemical + Disease Corpus) with BioBERT
---

Notebook to train/fine-tune a BioBERT model to perform named entity recognition (NER).

The [dataset](https://github.com/shreyashub/BioFLAIR/tree/master/data/ner/bc5cdr) used is a pre-processed version of the BC5CDR (BioCreative V CDR task corpus: a resource for  relation extraction) dataset from [Li et al. (2016)](https://pubmed.ncbi.nlm.nih.gov/27161011/).

#### Task Description

> Named entity recognition (NER) is the task of tagging entities in text with their corresponding type. Approaches typically use BIO notation, which differentiates the beginning (B) and the inside (I) of entities. O is used for non-entity tokens.

# Setting up the GPU Environment

#### Ensure we have a GPU runtime

If you're running this notebook in Google Colab, select `Runtime` > `Change Runtime Type` from the menubar. Ensure that `GPU` is selected as the `Hardware accelerator`. This will allow us to use the GPU to train the model subsequently.

#### Install Dependencies and Restart Runtime
"""

!pip install -q transformers
!pip install -q simpletransformers

"""You might see the error `ERROR: google-colab X.X.X has requirement ipykernel~=X.X, but you'll have ipykernel X.X.X which is incompatible` after installing the dependencies. **This is normal** and caused by the `simpletransformers` library.

The **solution** to this will be to **reset the execution environment** now. Go to the menu `Runtime` > `Restart runtime` then continue on from the next section to download and process the data.

# Getting Data

#### Pulling the data from Github

The dataset, includes train, test and dev sets, which we pull from the [Github repository](https://github.com/shreyashub/BioFLAIR/tree/master/data/ner/bc5cdr).
"""

import urllib.request
from pathlib import Path
from sklearn.metrics import classification_report

def download_file(url, output_file):
  Path(output_file).parent.mkdir(parents=True, exist_ok=True)
  urllib.request.urlretrieve (url, output_file)

download_file('https://raw.githubusercontent.com/shreyashub/BioFLAIR/master/data/ner/bc5cdr/train.txt', '/content/data/train.txt')
download_file('https://raw.githubusercontent.com/shreyashub/BioFLAIR/master/data/ner/bc5cdr/test.txt', '/content/data/test.txt')
download_file('https://raw.githubusercontent.com/shreyashub/BioFLAIR/master/data/ner/bc5cdr/dev.txt', '/content/data/dev.txt')

download_file('https://raw.githubusercontent.com/shreyashub/BioFLAIR/master/data/ner/NCBI-disease/train.txt', '/content/ncbi/data/train.txt')
download_file('https://raw.githubusercontent.com/shreyashub/BioFLAIR/master/data/ner/NCBI-disease/test.txt', '/content/ncbi/data/test.txt')
download_file('https://raw.githubusercontent.com/shreyashub/BioFLAIR/master/data/ner/NCBI-disease/dev.txt', '/content/ncbi/data/dev.txt')

"""Since the data is formatted in the CoNLL `BIO` type format (you can read more on the tagging format from this [wikipedia article](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))), we need to format it into a `pandas` dataframe with the following function. The 3 important columns in the dataframe are a word token (for mandarin this is a single character), a `BIO` label and a sentence_id to differentiate samples/sentences."""

import pandas as pd
def read_conll(filename):
    df = pd.read_csv(filename,
                    sep = '\t', header = None, keep_default_na = False,
                    names = ['words', 'pos', 'chunk', 'labels'],
                    quoting = 3, skip_blank_lines = False)
    df = df[~df['words'].astype(str).str.startswith('-DOCSTART- ')] # Remove the -DOCSTART- header
    df['sentence_id'] = (df.words == '').cumsum()
    return df[df.words != '']

"""Now we execute the function on the train, test and dev sets we have downloaded from Github. We also `.head()` the training set dataframe for the first 100 rows to check that the words, labels and sentence_id have been split properly."""

train_df = read_conll('/content/data/train.txt')
test_df = read_conll('/content/data/test.txt')
dev_df = read_conll('/content/data/dev.txt')
train_df.head(100)

"""We now print out the statistics (number of sentences) of the train, dev and test sets."""

data = [[train_df['sentence_id'].nunique(), test_df['sentence_id'].nunique(), dev_df['sentence_id'].nunique()]]

# Prints out the dataset sizes of train and test sets per label.
pd.DataFrame(data, columns=["Train", "Test", "Dev"])

"""# Training and Testing the Model

#### Set up the Training Arguments

We set up the training arguments. Here we train to 10 epochs to get accuracy close to the SOTA. The train, test and dev sets are relatively small so we don't have to wait too long. We set a sliding window as NER sequences can be quite long and because we have limited GPU memory we can't increase the `max_seq_length` too long.
"""

train_args = {
    'reprocess_input_data': True,
    'overwrite_output_dir': True,
    'sliding_window': True,
    'max_seq_length': 64,
    'num_train_epochs': 5,
    'train_batch_size': 32,
    'fp16': True,
    'output_dir': '/outputs/',
    'best_model_dir': '/outputs/best_model/',
    'evaluate_during_training': True,
}

"""The following line of code saves (to the variable `custom_labels`) a set of all the NER tags/labels in the dataset."""

custom_labels = list(train_df['labels'].unique())
print(custom_labels)

"""#### Train the Model
 We use the pre-trained BioBERT model (by [DMIS Lab, Korea University](https://huggingface.co/dmis-lab)) from the awesome [Hugging Face Transformers](https://github.com/huggingface/transformers) library as the base and use the [Simple Transformers library](https://simpletransformers.ai/docs/classification-models/) on top of it to make it so we can train the NER (sequence tagging) model with just a few lines of code.
"""

from simpletransformers.ner import NERModel
from transformers import AutoTokenizer
import pandas as pd
import logging

logging.basicConfig(level=logging.DEBUG)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

# We use the bio BERT pre-trained model.
BioBert_model = NERModel('bert', 'dmis-lab/biobert-v1.1', labels=custom_labels, args=train_args)

# Train the model
BioBert_model.train_model(train_df, eval_data=dev_df)

# Evaluate the model in terms of accuracy score
result1, model_outputs1, preds_list1 = BioBert_model.eval_model(test_df)

# Print the F1-score
print(f"\n\nF1-score: {result1['f1_score']}")

sample = test_df[test_df.sentence_id == 10].words.str.cat(sep=' ')
print(sample)

samples = [sample]
predictions, _ = BioBert_model.predict(samples)
for idx, sample in enumerate(samples):
  print('{}: '.format(idx))
  for word in predictions[idx]:
    print('{}'.format(word))

# from google.colab import drive
# drive.mount('/content/drive/')

"""You can move the model checkpount files which are saved in the `/outputs/` directory to your Google Drive.

# **Bert**
"""

Bert_model = NERModel('bert', 'bert-base-uncased', labels=custom_labels, args=train_args)

# Train the model
Bert_model.train_model(train_df, eval_data=dev_df)

# Evaluate the model in terms of accuracy score
result2, model_outputs2, preds_list2 = Bert_model.eval_model(test_df)

# Print the F1-score
print(f"\n\nF1-score: {result2['f1_score']}")

sample = test_df[test_df.sentence_id == 10].words.str.cat(sep=' ')
print(sample)

samples = [sample]
predictions, _ = Bert_model.predict(samples)
for idx, sample in enumerate(samples):
  print('{}: '.format(idx))
  for word in predictions[idx]:
    print('{}'.format(word))

"""# **RoBERTa**"""

# Example 4: RoBERTa
RoBerta_model = NERModel('roberta', 'roberta-base', labels=custom_labels, args=train_args)

# Train the model
RoBerta_model.train_model(train_df, eval_data=dev_df)

# Evaluate the model in terms of accuracy score
result3, model_outputs3, preds_list3 = RoBerta_model.eval_model(test_df)

# Print the F1-score
print(f"\n\nF1-score: {result3['f1_score']}")

samples = [sample]
predictions, _ = RoBerta_model.predict(samples)
for idx, sample in enumerate(samples):
  print('{}: '.format(idx))
  for word in predictions[idx]:
    print('{}'.format(word))

"""# **SciBERT**"""

# Example 5: SciBERT
SciBert_model = NERModel('bert', 'allenai/scibert_scivocab_uncased', labels=custom_labels, args=train_args)

# Train the model
SciBert_model.train_model(train_df, eval_data=dev_df)

# Evaluate the model in terms of accuracy score
result4, model_outputs4, preds_list4 = SciBert_model.eval_model(test_df)

# Print the F1-score
print(f"\n\nF1-score: {result4['f1_score']}")

samples = [sample]
predictions, _ = SciBert_model.predict(samples)
for idx, sample in enumerate(samples):
  print('{}: '.format(idx))
  for word in predictions[idx]:
    print('{}'.format(word))

"""# **SpanBERT**"""

# Example 6: SpanBERT
SpanBert_model = NERModel('bert', 'SpanBERT/spanbert-base-cased', labels=custom_labels, args=train_args)

# Train the model
SpanBert_model.train_model(train_df, eval_data=dev_df)

# Evaluate the model in terms of accuracy score
result5, model_outputs5, preds_list5 = SpanBert_model.eval_model(test_df)

# Print the F1-score
print(f"\n\nF1-score: {result5['f1_score']}")

samples = [sample]
predictions, _ = SpanBert_model.predict(samples)
for idx, sample in enumerate(samples):
  print('{}: '.format(idx))
  for word in predictions[idx]:
    print('{}'.format(word))

"""# **PubMedBERT**"""

# Example 8: PubMedBERT
PubMedBert_model = NERModel('bert', 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext', labels=custom_labels, args=train_args)

# Train the model
PubMedBert_model.train_model(train_df, eval_data=dev_df)

# Evaluate the model in terms of accuracy score
result6, model_outputs6, preds_list6 = PubMedBert_model.eval_model(test_df)

# Print the F1-score
print(f"\n\nF1-score: {result6['f1_score']}")

samples = [sample]
predictions, _ = PubMedBert_model.predict(samples)
for idx, sample in enumerate(samples):
  print('{}: '.format(idx))
  for word in predictions[idx]:
    print('{}'.format(word))

from simpletransformers.ner import NERModel
from transformers import AutoTokenizer
import pandas as pd
import logging
import numpy as np

logging.basicConfig(level=logging.DEBUG)
transformers_logger = logging.getLogger('transformers')
transformers_logger.setLevel(logging.WARNING)

model_names = [
    ('bert', 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext'),
    ('bert', 'allenai/scibert_scivocab_uncased'),
    ('bert', 'dmis-lab/biobert-v1.1')
]
models = []

for archi, model_name in model_names:
    model = NERModel(archi, model_name, labels=custom_labels, args=train_args)
    model.train_model(train_df, eval_data=dev_df)
    models.append(model)

# Make predictions using each model
all_predictions = []

for model in models:
    predictions, _ = model.predict(test_df['words'].tolist())
    all_predictions.append(predictions)

# Ensemble by majority voting (you can also use averaging or other strategies)
ensemble_predictions = np.array(all_predictions).transpose(1, 0, 2)
final_predictions = []

for sentence_predictions in ensemble_predictions:
    # Perform majority voting for each token
    majority_votes = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=sentence_predictions)
    final_predictions.extend(majority_votes)

# Convert predictions to a flat list
flat_predictions = [label for sentence_labels in final_predictions for label in sentence_labels]

# Convert true labels to a flat list
flat_true_labels = [label for sentence_labels in test_df['labels'].tolist() for label in sentence_labels.split()]

# Calculate F1-score
from sklearn.metrics import f1_score
# f1 = f1_score(flat_true_labels, flat_predictions, average='weighted')

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, f1_score

# Initialize lists to store metrics for each model
model_names = []
f1_scores = []
accuracy_scores = []

# Make predictions and calculate metrics for each model
for model_name, model in zip(model_names, models):
    predictions, _ = model.predict(test_df['words'].tolist())

    # Ensemble by majority voting (you can also use averaging or other strategies)
    ensemble_predictions = np.array(predictions).transpose(1, 0, 2)
    final_predictions = []

    for sentence_predictions in ensemble_predictions:
        # Perform majority voting for each token
        majority_votes = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=sentence_predictions)
        final_predictions.extend(majority_votes)

    # Convert predictions to a flat list
    flat_predictions = [label for sentence_labels in final_predictions for label in sentence_labels]

    # Convert true labels to a flat list
    flat_true_labels = [label for sentence_labels in test_df['labels'].tolist() for label in sentence_labels.split()]

    # Calculate F1-score and accuracy
    f1 = f1_score(flat_true_labels, flat_predictions, average='weighted')
    accuracy = accuracy_score(flat_true_labels, flat_predictions)

    # Append metrics to lists
    model_names.append(model_name)
    f1_scores.append(f1)
    accuracy_scores.append(accuracy)

# Plot the results
plt.figure(figsize=(10, 6))

plt.subplot(1, 2, 1)
plt.bar(model_names, f1_scores, color='blue')
plt.title('F1-score for Each Model')
plt.xlabel('Model')
plt.ylabel('F1-score')

plt.subplot(1, 2, 2)
plt.bar(model_names, accuracy_scores, color='green')
plt.title('Accuracy for Each Model')
plt.xlabel('Model')
plt.ylabel('Accuracy')

plt.tight_layout()
plt.show()

"""**Sample**"""

# Sample string to test
sample_text = "This is a sample sentence for testing the NER models."

# Make predictions using each model
all_predictions1 = []

for model in models:
    predictions, _ = model.predict([sample_text])
    all_predictions1.append(predictions)

# Ensemble by majority voting (you can also use averaging or other strategies)
ensemble_predictions1 = np.array(all_predictions1).transpose(1, 0, 2)[0]
final_predictions1 = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=ensemble_predictions1)

# Print the final predictions for the sample string
print(f"Sample Text: {sample_text}")
print("Ensemble Predictions:", final_predictions1)

ensemble_predictions

